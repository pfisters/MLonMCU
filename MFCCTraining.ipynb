{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction and Network training\n",
    "\n",
    "In this notebook you will go through an example flow of processing audio data, complete with feature extraction and training. Then we will use TFLite to quantize the model to int8 and process to be able to port it over to the MCU.\n",
    "\n",
    "Make sure you read the instructions on the exercise sheet and follow the task order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hey_snips_research_6k_en_train_eval_clean_ter/train.json'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-352c8d069d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mDataSetPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hey_snips_research_6k_en_train_eval_clean_ter/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"train.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjsonfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtraindata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hey_snips_research_6k_en_train_eval_clean_ter/train.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tqdm import tqdm\n",
    "\n",
    "DataSetPath = \"hey_snips_research_6k_en_train_eval_clean_ter/\"\n",
    "\n",
    "with open(DataSetPath+\"train.json\") as jsonfile:\n",
    "    traindata = json.load(jsonfile)\n",
    "\n",
    "with open(DataSetPath+\"test.json\") as jsonfile:\n",
    "    testdata = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "\n",
    "    x_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    totalSliceLength = 10 # Length to stuff the signals to, given in seconds\n",
    "\n",
    "    # trainsize = len(traindata) # Number of loaded training samples\n",
    "    # testsize = len(testdata) # Number of loaded testing samples\n",
    "\n",
    "    trainsize = 1000 # Number of loaded training samples\n",
    "    testsize = 100 # Number of loaded testing samples\n",
    "\n",
    "\n",
    "    fs = 16000 # Sampling rate of the samples\n",
    "    segmentLength = 1024 # Number of samples to use per segment\n",
    "\n",
    "    sliceLength = int(totalSliceLength * fs / segmentLength)*segmentLength\n",
    "\n",
    "    for i in tqdm(range(trainsize)): \n",
    "        fs, train_sound_data = wavfile.read(DataSetPath+traindata[i]['audio_file_path']) # Read wavfile to extract amplitudes\n",
    "\n",
    "        _x_train = train_sound_data.copy() # Get a mutable copy of the wavfile\n",
    "        _x_train.resize(sliceLength) # Zero stuff the single to a length of sliceLength\n",
    "        _x_train = _x_train.reshape(-1,int(segmentLength)) # Split slice into Segments with 0 overlap\n",
    "        x_train_list.append(_x_train.astype(np.float32)) # Add segmented slice to training sample list, cast to float so librosa doesn't complain\n",
    "        y_train_list.append(traindata[i]['is_hotword']) # Read label \n",
    "\n",
    "    for i in tqdm(range(testsize)):\n",
    "        fs, test_sound_data = wavfile.read(DataSetPath+testdata[i]['audio_file_path'])\n",
    "        _x_test = test_sound_data.copy()\n",
    "        _x_test.resize(sliceLength)\n",
    "        _x_test = _x_test.reshape((-1,int(segmentLength)))\n",
    "        x_test_list.append(_x_test.astype(np.float32))\n",
    "        y_test_list.append(testdata[i]['is_hotword'])\n",
    "\n",
    "    x_train = tf.convert_to_tensor(np.asarray(x_train_list))\n",
    "    y_train = tf.convert_to_tensor(np.asarray(y_train_list))\n",
    "\n",
    "    x_test = tf.convert_to_tensor(np.asarray(x_test_list))\n",
    "    y_test = tf.convert_to_tensor(np.asarray(y_test_list))\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfccs(tensor):\n",
    "    sample_rate = 16000.0\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    frame_length = 1024\n",
    "    num_mfcc = 13\n",
    "\n",
    "    stfts = tf.signal.stft(tensor, frame_length=frame_length, frame_step=frame_length, fft_length=frame_length)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    spectrograms = tf.reshape(spectrograms, (spectrograms.shape[0],spectrograms.shape[1],-1))\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfcc]\n",
    "    return tf.reshape(mfccs, (mfccs.shape[0],mfccs.shape[1],mfccs.shape[2],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mfcc = compute_mfccs(x_train)\n",
    "x_test_mfcc = compute_mfccs(x_test)\n",
    "\n",
    "print(x_train_mfcc.shape)\n",
    "print(x_test_mfcc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "epochs = 30\n",
    "\n",
    "train_set = (x_train_mfcc/512 + 0.5)\n",
    "train_labels = y_train\n",
    "\n",
    "test_set = (x_test_mfcc/512 + 0.5)\n",
    "test_labels = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.2609 - accuracy: 0.9310\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1170 - accuracy: 0.9530\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0884 - accuracy: 0.9690\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0778 - accuracy: 0.9740\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0501 - accuracy: 0.9840\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0549 - accuracy: 0.9820\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0406 - accuracy: 0.9870\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0330 - accuracy: 0.9890\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0350 - accuracy: 0.9880\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0287 - accuracy: 0.9910\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0213 - accuracy: 0.9940\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0273 - accuracy: 0.9920\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0194 - accuracy: 0.9950\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0179 - accuracy: 0.9930\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0219 - accuracy: 0.9930\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0176 - accuracy: 0.9940\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0140 - accuracy: 0.9930\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0141 - accuracy: 0.9950\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0141 - accuracy: 0.9950\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0102 - accuracy: 0.9960\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0080 - accuracy: 0.9980\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0162 - accuracy: 0.9930\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0108 - accuracy: 0.9950\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0127 - accuracy: 0.9930\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0085 - accuracy: 0.9980\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0083 - accuracy: 0.9960\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0060 - accuracy: 0.9990\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0051 - accuracy: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0f7a9c9d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(layers.InputLayer(input_shape=(train_set.shape[1],train_set.shape[2],train_set.shape[3]), batch_size=(batchSize)))\n",
    "model.add(layers.Conv2D(filters=3,kernel_size=(3,3),padding=\"same\",input_shape=(train_set[0].shape)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Conv2D(filters=16,kernel_size=(3,3),strides=(2,2),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32,kernel_size=(3,3),strides=(2,2),padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=48,kernel_size=(3,3),padding='same',strides=(2,2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(8, kernel_regularizer=(regularizers.l1(0))))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Dense(2))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(train_set, y_train, batchSize, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (10, 156, 13, 3)          30        \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (10, 156, 13, 3)          12        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (10, 156, 13, 3)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (10, 78, 7, 16)           448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (10, 78, 7, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (10, 78, 7, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (10, 39, 3, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (10, 20, 2, 32)           4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (10, 20, 2, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (10, 20, 2, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (10, 10, 1, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (10, 5, 1, 48)            13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (10, 5, 1, 48)            192       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (10, 5, 1, 48)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (10, 48)                  0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (10, 48)                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (10, 8)                   392       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (10, 8)                   0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (10, 2)                   18        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (10, 2)                   0         \n",
      "=================================================================\n",
      "Total params: 19,796\n",
      "Trainable params: 19,598\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 2.3750\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "score = model.evaluate(test_set, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MFCCmodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFLite conversion\n",
    "#### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thoriri/miniconda3/envs/ml-on-mcu/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/thoriri/miniconda3/envs/ml-on-mcu/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpwuowpnbt/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29344"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = train_set.numpy()\n",
    "test_set = test_set.numpy()\n",
    "train_labels = train_labels.numpy()\n",
    "test_labels = test_labels.numpy()\n",
    "tflite_model_name = 'MFCC'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for i in range(500):\n",
    "            yield([train_set[i].reshape(1,156,13,1)])\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function here takes in the model and outputs an header file we will import into the TFLite example project folder. (/Core/Inc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'MFCC'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's have a look at the network we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: conv2d_input\n",
      "shape: [  1 156  13   1]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: Identity\n",
      "shape: [1 2]\n",
      "type: <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next let's see how the performance of the network is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((len(test_set),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(test_set)):\n",
    "    val_batch = test_set[i]\n",
    "    #We must convert the data into int8 format before invoking inference.\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of quantized to int8 model is 99.0%\n",
      "Compared to float32 accuracy of 98.00000190734863%\n",
      "We have a change of 0.9999980926513663%\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 100\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(score[1]*100))\n",
    "print(\"We have a change of {}%\".format((accuracy_score-score[1])*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('MLonMCU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7ee7167f8cdf451c85868564889205e11a305ff6aad56fb52329bfecd5b1c66e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}